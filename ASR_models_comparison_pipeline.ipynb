{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5190a371",
   "metadata": {},
   "source": [
    "## Основной пайплан транскрибации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6ef60e",
   "metadata": {},
   "source": [
    "### Транскрибация и расчет метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cd5292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import whisper\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from pydub import AudioSegment\n",
    "from IPython.display import display\n",
    "from ipywidgets import widgets, Button, VBox, Label\n",
    "from pyannote.audio import Pipeline\n",
    "import traceback\n",
    "from jiwer import wer, cer, mer, wil\n",
    "from pymystem3 import Mystem\n",
    "import numpy as np\n",
    "import json\n",
    "import wave\n",
    "from vosk import Model, KaldiRecognizer\n",
    "from transformers import AutoProcessor, SeamlessM4Tv2Model, Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import torchaudio\n",
    "import gc\n",
    "from scipy.io import wavfile\n",
    "import noisereduce as nr\n",
    "import librosa\n",
    "import gigaam\n",
    "\n",
    "\n",
    "AUTH_TOKEN = #Токен higging face. Получить бесплатно можно тут https://huggingface.co/settings/tokens\n",
    "os.environ[\"HF_TOKEN\"] = AUTH_TOKEN\n",
    "\n",
    "\n",
    "DIARIZATION_MODEL = \"pyannote/speaker-diarization-3.1\"\n",
    "NUM_SPEAKERS = 2\n",
    "\n",
    "mystem = Mystem()\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def clear_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def normalize_text(text):\n",
    "\n",
    "    # text = text.replace('SPEAKER_00', ' ')\n",
    "    # text = text.replace('SPEAKER_01', ' ')\n",
    "    # text = text.replace('UNKNOWN', ' ') \n",
    "    text = text.lower().replace('ё', 'е')\n",
    "    text = re.sub(r'[-–]', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    try:\n",
    "        return ''.join(mystem.lemmatize(text)).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка лемматизации: {str(e)}\")\n",
    "        return text\n",
    "\n",
    "def get_audio_duration(file_path):\n",
    "    audio = AudioSegment.from_file(file_path)\n",
    "    return len(audio) / 1000.0\n",
    "\n",
    "def calculate_metrics(reference, hypothesis):\n",
    "    def safe_lemmatize(text):\n",
    "        try:\n",
    "            return lemmatize_text(text) if text else \"\"\n",
    "        except:\n",
    "            return text\n",
    "    \n",
    "    ref_norm = normalize_text(reference or \"\")\n",
    "    hyp_norm = normalize_text(hypothesis or \"\")\n",
    "    ref_lemma = safe_lemmatize(ref_norm)\n",
    "    hyp_lemma = safe_lemmatize(hyp_norm)\n",
    "\n",
    "    def calculate_level_metrics(ref, hyp):\n",
    "        if not ref or not hyp:\n",
    "            return {'WER': 1.0, 'CER': 1.0, 'MER': 1.0, 'WIL': 1.0, 'PIWER': 1.0}\n",
    "        \n",
    "        return {\n",
    "            'WER': wer(ref, hyp),\n",
    "            'CER': cer(ref, hyp),\n",
    "            'MER': mer(ref, hyp),\n",
    "            'WIL': wil(ref, hyp),\n",
    "            'PIWER': 1 - len(set(ref.split()) & set(hyp.split())) / max(1, len(set(ref.split())))\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'Base': calculate_level_metrics(ref_norm, hyp_norm),\n",
    "        'Lemmatized': calculate_level_metrics(ref_lemma, hyp_lemma)\n",
    "    }\n",
    "\n",
    "def convert_to_wav(file_path):\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(file_path)\n",
    "        wav_path = os.path.splitext(file_path)[0] + \".wav\"\n",
    "        audio.export(wav_path, format=\"wav\", parameters=[\"-ac\", \"1\", \"-ar\", \"16000\"])\n",
    "        return wav_path\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Ошибка конвертации: {str(e)}\")\n",
    "\n",
    "def apply_audio_preprocessing(file_path):\n",
    "    try:\n",
    "        rate, data = wavfile.read(file_path)\n",
    "        \n",
    "        if len(data.shape) > 1:\n",
    "            data = data.mean(axis=1)\n",
    "            \n",
    "        data = data.astype(np.float32)\n",
    "        data /= np.max(np.abs(data))\n",
    "        \n",
    "        reduced_noise = nr.reduce_noise(\n",
    "            y=data,\n",
    "            sr=rate,\n",
    "            stationary=True,\n",
    "            prop_decrease=0.75\n",
    "        )\n",
    "        \n",
    "        trimmed, _ = librosa.effects.trim(reduced_noise, top_db=20)\n",
    "        \n",
    "        temp_path = f\"preprocessed_{os.path.basename(file_path)}\"\n",
    "        wavfile.write(temp_path, rate, trimmed)\n",
    "        \n",
    "        return temp_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Ошибка предобработки: {str(e)}\")\n",
    "\n",
    "def transcribe_whisper(file_path, model_size):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    try:\n",
    "        clear_memory()\n",
    "        start_time = time.time()\n",
    "        model = whisper.load_model(model_size, device=device)\n",
    "        \n",
    "        audio = whisper.load_audio(file_path)\n",
    "        result = model.transcribe(\n",
    "            audio,\n",
    "            word_timestamps=True,\n",
    "            language=\"ru\",\n",
    "            initial_prompt=\"Запиши все числительные словами, а не цифрами.\" # промт для корректной записи числительных\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'text': result['text'],\n",
    "            'segments': result['segments'],\n",
    "            'time': time.time() - start_time,\n",
    "            'device': device.upper()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Whisper: ошибка {str(e)}\")\n",
    "    finally:\n",
    "        del model\n",
    "        clear_memory()\n",
    "\n",
    "def transcribe_whisper_medium(file_path):\n",
    "    return transcribe_whisper(file_path, \"medium\")\n",
    "\n",
    "def transcribe_whisper_large(file_path):\n",
    "    return transcribe_whisper(file_path, \"large-v3-turbo\")\n",
    "\n",
    "def transcribe_vosk(file_path):\n",
    "    device = 'cpu'\n",
    "    try:\n",
    "        clear_memory()\n",
    "        start_time = time.time()\n",
    "        model_path = \"vosk-model-ru-0.42\"\n",
    "        if not os.path.exists(model_path):\n",
    "            raise RuntimeError(f\"Модель Vosk не найдена в корне папки: {model_path}\")\n",
    "            \n",
    "        model = Model(model_path)\n",
    "        wf = wave.open(file_path, \"rb\")\n",
    "        rec = KaldiRecognizer(model, wf.getframerate())\n",
    "        rec.SetWords(True)\n",
    "        \n",
    "        results = []\n",
    "        while True:\n",
    "            data = wf.readframes(4000)\n",
    "            if not data: break\n",
    "            if rec.AcceptWaveform(data):\n",
    "                results.append(json.loads(rec.Result()))\n",
    "        \n",
    "        results.append(json.loads(rec.FinalResult()))\n",
    "        wf.close()\n",
    "\n",
    "        segments = []\n",
    "        full_text = []\n",
    "        for res in results:\n",
    "            if 'result' in res:\n",
    "                for word in res['result']:\n",
    "                    segments.append({\n",
    "                        'start': word['start'],\n",
    "                        'end': word['end'],\n",
    "                        'text': word['word']\n",
    "                    })\n",
    "                    full_text.append(word['word'])\n",
    "            elif 'text' in res and res['text']:\n",
    "                segments.append({\n",
    "                    'start': 0,\n",
    "                    'end': get_audio_duration(file_path),\n",
    "                    'text': res['text']\n",
    "                })\n",
    "                full_text.append(res['text'])\n",
    "\n",
    "        return {\n",
    "            'text': ' '.join(full_text),\n",
    "            'segments': segments,\n",
    "            'time': time.time() - start_time,\n",
    "            'device': device.upper()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Vosk: ошибка {str(e)}\")\n",
    "    finally:\n",
    "        del model\n",
    "        clear_memory()\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def transcribe_seamless(file_path):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    try:\n",
    "        clear_memory()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        processor = AutoProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n",
    "        model = SeamlessM4Tv2Model.from_pretrained(\n",
    "            \"facebook/seamless-m4t-v2-large\",\n",
    "            device_map=device\n",
    "        ).eval()\n",
    "        \n",
    "        audio_data, sample_rate = torchaudio.load(file_path)\n",
    "        audio_data = audio_data.mean(dim=0)  \n",
    "        total_duration = audio_data.shape[0] / sample_rate\n",
    "        \n",
    "\n",
    "        chunk_duration = 30  \n",
    "        chunk_size = int(chunk_duration * sample_rate)\n",
    "        overlap = int(0.5 * sample_rate)  \n",
    "        \n",
    "        segments = []\n",
    "        full_text = []\n",
    "        current_pos = 0\n",
    "        \n",
    "        while current_pos < len(audio_data):\n",
    "            chunk_start = int(max(0, current_pos - overlap) if current_pos > 0 else 0)\n",
    "            chunk_end = int(min(current_pos + chunk_size, len(audio_data)))\n",
    "            \n",
    "            chunk = audio_data[chunk_start:chunk_end]\n",
    "            \n",
    "            chunk = chunk / torch.max(torch.abs(chunk))\n",
    "            \n",
    "            inputs = processor(\n",
    "                audios=chunk.numpy(),\n",
    "                return_tensors=\"pt\",\n",
    "                sampling_rate=sample_rate\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    tgt_lang=\"rus\",\n",
    "                    generate_speech=False\n",
    "                )\n",
    "                \n",
    "                if hasattr(outputs, 'text'):\n",
    "                    text = outputs.text[0]\n",
    "                else:\n",
    "                    text = processor.batch_decode(outputs.sequences, skip_special_tokens=True)[0]\n",
    "            \n",
    "            segment_start = chunk_start / sample_rate\n",
    "            segment_end = chunk_end / sample_rate\n",
    "            \n",
    "            segments.append({\n",
    "                'start': segment_start,\n",
    "                'end': segment_end,\n",
    "                'text': text\n",
    "            })\n",
    "            \n",
    "            full_text.append(text)\n",
    "            current_pos += chunk_size\n",
    "            \n",
    "            del inputs, outputs\n",
    "            clear_memory()\n",
    "        \n",
    "        return {\n",
    "            'text': ' '.join(full_text),\n",
    "            'segments': segments,\n",
    "            'time': time.time() - start_time,\n",
    "            'device': device.upper()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Seamless-M4T: ошибка {str(e)}\\n{traceback.format_exc()}\")\n",
    "    finally:\n",
    "        del model\n",
    "        clear_memory()\n",
    "\n",
    "\n",
    "def transcribe_wav2vec(file_path):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    try:\n",
    "        clear_memory()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        processor = Wav2Vec2Processor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-russian\")\n",
    "        model = Wav2Vec2ForCTC.from_pretrained(\n",
    "            \"jonatasgrosman/wav2vec2-large-xlsr-53-russian\"\n",
    "        ).to(device).eval()\n",
    "        \n",
    "\n",
    "        audio_data, sample_rate = torchaudio.load(file_path)\n",
    "        audio_data = audio_data.mean(dim=0)  \n",
    "        \n",
    "\n",
    "        total_duration = audio_data.shape[0] / sample_rate\n",
    "        \n",
    "\n",
    "        chunk_duration = 30 \n",
    "        chunk_size = int(chunk_duration * sample_rate)\n",
    "        overlap = int(0.5 * sample_rate) \n",
    "        \n",
    "        segments = []\n",
    "        full_text = []\n",
    "        current_pos = 0\n",
    "        \n",
    "        while current_pos < len(audio_data):\n",
    "            chunk_start = max(0, current_pos - overlap) if current_pos > 0 else 0\n",
    "            chunk_end = min(current_pos + chunk_size, len(audio_data))\n",
    "            \n",
    "            chunk = audio_data[chunk_start:chunk_end]\n",
    "\n",
    "            chunk = chunk / torch.max(torch.abs(chunk))\n",
    "\n",
    "            input_values = processor(\n",
    "                chunk.numpy(), \n",
    "                sampling_rate=sample_rate, \n",
    "                return_tensors=\"pt\"\n",
    "            ).input_values.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = model(input_values).logits\n",
    "                predicted_ids = torch.argmax(logits, dim=-1)\n",
    "                text = processor.batch_decode(predicted_ids)[0]\n",
    "            \n",
    "            segment_start = chunk_start / sample_rate\n",
    "            segment_end = chunk_end / sample_rate\n",
    "            \n",
    "            segments.append({\n",
    "                'start': segment_start,\n",
    "                'end': segment_end,\n",
    "                'text': text\n",
    "            })\n",
    "            \n",
    "            full_text.append(text)\n",
    "            current_pos += chunk_size - overlap  \n",
    "            \n",
    "\n",
    "            del input_values, logits, predicted_ids\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return {\n",
    "            'text': ' '.join(full_text),\n",
    "            'segments': segments,\n",
    "            'time': time.time() - start_time,\n",
    "            'device': device.upper(),\n",
    "            'total_duration': total_duration\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Wav2Vec2 ошибка: {str(e)}\\n{traceback.format_exc()}\")\n",
    "    finally:\n",
    "        del model\n",
    "        clear_memory()\n",
    "\n",
    "def transcribe_gigaam(file_path):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu' # Тут алгортм выбирает cuda автоматически. Попытка прописать cuda руками, приводит к конфлитку версий библиотек.\n",
    "    try:\n",
    "        clear_memory()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model = gigaam.load_model(\"v2_rnnt\")  # Другие подвиды модели: \"v2_ctc\" or \"ctc\", \"v2_rnnt\" or \"rnnt\", \"v1_ctc\", \"v1_rnnt\"\n",
    "        # v2_rnnt - является лучшей\n",
    "\n",
    "        recognition_result = model.transcribe_longform(file_path)\n",
    "        \n",
    "\n",
    "        full_text = ' '.join([segment['transcription'] for segment in recognition_result])\n",
    "        \n",
    "\n",
    "        segments = []\n",
    "        for segment in recognition_result:\n",
    "            segments.append({\n",
    "                'start': segment['boundaries'][0],\n",
    "                'end': segment['boundaries'][1],\n",
    "                'text': segment['transcription']\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'text': full_text,\n",
    "            'segments': segments,\n",
    "            'time': time.time() - start_time,\n",
    "            'device': device.upper()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"GigaAM error: {str(e)}\\n{traceback.format_exc()}\")\n",
    "    finally:\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        clear_memory()\n",
    "\n",
    "def diarize_audio(file_path):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    try:\n",
    "        clear_memory()\n",
    "        start_time = time.time()\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            DIARIZATION_MODEL,\n",
    "            use_auth_token=AUTH_TOKEN\n",
    "        ).to(torch.device(device))\n",
    "        \n",
    "        diarization = pipeline(file_path, num_speakers=NUM_SPEAKERS)\n",
    "        \n",
    "        return {\n",
    "            'diarization': diarization,\n",
    "            'time': time.time() - start_time,\n",
    "            'device': device.upper()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Ошибка диаризации: {str(e)}\")\n",
    "    finally:\n",
    "        del pipeline\n",
    "        clear_memory()\n",
    "\n",
    "def match_speakers(transcript_segments, diarization):\n",
    "    matched = []\n",
    "    for seg in transcript_segments:\n",
    "        best_speaker = \"UNKNOWN\"\n",
    "        max_overlap = 0\n",
    "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "            overlap_start = max(seg['start'], turn.start)\n",
    "            overlap_end = min(seg['end'], turn.end)\n",
    "            if overlap_start < overlap_end:\n",
    "                overlap = overlap_end - overlap_start\n",
    "                if overlap > max_overlap:\n",
    "                    max_overlap = overlap\n",
    "                    best_speaker = speaker\n",
    "        matched.append({\n",
    "            'start': seg['start'],\n",
    "            'end': seg['end'],\n",
    "            'speaker': best_speaker,\n",
    "            'text': seg['text'].strip()\n",
    "        })\n",
    "    return matched\n",
    "\n",
    "def save_combined_results(file_name, all_results, diarize_time, audio_duration, preprocessing_enabled=False):\n",
    "    output_file = f\"{os.path.splitext(file_name)[0]}_combined_report.txt\"\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"СРАВНИТЕЛЬНЫЙ ОТЧЕТ\\n\")\n",
    "        f.write(\"###############################################\\n\\n\")\n",
    "        f.write(f\"Файл: {file_name}\\n\")\n",
    "        f.write(f\"Предобработка аудио: {'включена' if preprocessing_enabled else 'выключена'}\\n\")\n",
    "        f.write(f\"Длительность записи: {audio_duration:.1f} сек\\n\")\n",
    "        f.write(f\"Время диаризации: {diarize_time:.1f} сек\\n\")\n",
    "        f.write(f\"Устройство диаризации: {all_results.get('diarization', {}).get('device', 'N/A')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"МЕТРИКИ КАЧЕСТВА:\\n\")\n",
    "        f.write(\"{:<25} {:<10} {:<10} {:<10} {:<10} {:<10} {:<15} {:<15} {:<15}\\n\".format(\n",
    "            \"Модель\", \"WER\", \"CER\", \"MER\", \"WIL\", \"PIWER\", \"RTF\", \"Время (сек)\", \"Устройство\"))\n",
    "        \n",
    "        for model_name, results in all_results.items():\n",
    "            if model_name == 'diarization' or 'error' in results:\n",
    "                continue\n",
    "                \n",
    "            time_str = f\"{results.get('time', 'N/A'):.1f}\" if 'time' in results else 'N/A'\n",
    "            metrics = results.get('metrics')\n",
    "            device = results.get('device', 'N/A')\n",
    "            \n",
    "            if metrics:\n",
    "                f.write(\"{:<25} {:<10.2%} {:<10.2%} {:<10.2%} {:<10.2%} {:<10.2%} {:<15} {:<15} {:<15}\\n\".format(\n",
    "                    f\"\\n {model_name} \\n  → Base                 \",\n",
    "                    metrics['Base']['WER'],\n",
    "                    metrics['Base']['CER'],\n",
    "                    metrics['Base']['MER'],\n",
    "                    metrics['Base']['WIL'],\n",
    "                    metrics['Base']['PIWER'],\n",
    "                    round(float(time_str)/audio_duration, 2),\n",
    "                    time_str,\n",
    "                    device\n",
    "                ))\n",
    "                f.write(\"{:<25} {:<10.2%} {:<10.2%} {:<10.2%} {:<10.2%} {:<10.2%} {:<15} {:<15} {:<15}\\n\".format(\n",
    "                    \"  → Lemmatized\",\n",
    "                    metrics['Lemmatized']['WER'],\n",
    "                    metrics['Lemmatized']['CER'],\n",
    "                    metrics['Lemmatized']['MER'],\n",
    "                    metrics['Lemmatized']['WIL'],\n",
    "                    metrics['Lemmatized']['PIWER'],\n",
    "                    \"\",\n",
    "                    \"\",\n",
    "                    \"\"\n",
    "                ))\n",
    "            else:\n",
    "                f.write(\"{:<25} {:<10} {:<10} {:<10} {:<15} {:<15} {:<15}\\n\".format(\n",
    "                    model_name, \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", round(float(time_str)/audio_duration, 2), time_str, device))\n",
    "\n",
    "        for model_name, results in all_results.items():\n",
    "            if model_name == 'diarization':\n",
    "                continue\n",
    "                \n",
    "            f.write(\"\\n\\n###############################################\\n\")\n",
    "            if 'error' in results:\n",
    "                f.write(f\"МОДЕЛЬ: {model_name} - ОШИБКА\\n{results['error']}\\n\")\n",
    "                continue\n",
    "                \n",
    "            f.write(f\"МОДЕЛЬ: {model_name}\\n\")\n",
    "            f.write(f\"Устройство обработки: {results.get('device', 'N/A')}\\n\")\n",
    "            if 'time' in results:\n",
    "                f.write(f\"Время обработки: {results['time']:.1f} сек\\n\")\n",
    "            \n",
    "            if 'segments' in results and results['segments']:\n",
    "                f.write(\"\\nТРАНСКРИПТ:\\n\")\n",
    "                current_speaker = None\n",
    "                for entry in results['segments']:\n",
    "                    if entry['speaker'] != current_speaker:\n",
    "                        f.write(f\"\\n[{entry['speaker']}]:\\n\")\n",
    "                        current_speaker = entry['speaker']\n",
    "                    f.write(f\"{entry['text']} \")\n",
    "            else:\n",
    "                f.write(\"\\nТранскрипт недоступен\\n\")\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "audio_uploader = widgets.FileUpload(accept='audio/*', multiple=False)\n",
    "reference_uploader = widgets.FileUpload(accept='text/*', multiple=False)\n",
    "process_btn = Button(description=\"Начать обработку\", button_style='success')\n",
    "status_label = Label(value='')\n",
    "preprocessing_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Предобработка аудио',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "\n",
    "audio_uploader = widgets.FileUpload(accept='audio/*', multiple=False)\n",
    "reference_uploader = widgets.FileUpload(accept='text/*', multiple=False)\n",
    "process_btn = Button(description=\"Начать обработку\", button_style='success')\n",
    "status_label = Label(value='')\n",
    "preprocessing_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Предобработка аудио',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "model_checkboxes = {\n",
    "    'Whisper Medium': widgets.Checkbox(value=True, description='Whisper Medium'),\n",
    "    'Whisper Large-v3-turbo': widgets.Checkbox(value=True, description='Whisper Large-v3-turbo'),\n",
    "    'Vosk 0.42 RU': widgets.Checkbox(value=True, description='Vosk 0.42 RU'),\n",
    "    'Seamless-M4T-v2-large': widgets.Checkbox(value=True, description='Seamless-M4T-v2-large'),\n",
    "    'Wav2Vec-finetuned': widgets.Checkbox(value=True, description='Wav2Vec-finetuned'),\n",
    "    'GigaAM': widgets.Checkbox(value=True, description='GigaAM')\n",
    "}\n",
    "\n",
    "models_mapping = {\n",
    "    'Whisper Medium': transcribe_whisper_medium,\n",
    "    'Whisper Large-v3-turbo': transcribe_whisper_large,\n",
    "    'Vosk 0.42 RU': transcribe_vosk,\n",
    "    'Seamless-M4T-v2-large': transcribe_seamless,\n",
    "    'Wav2Vec-finetuned': transcribe_wav2vec,\n",
    "    'GigaAM': transcribe_gigaam\n",
    "}\n",
    "\n",
    "model_selection = VBox([\n",
    "    Label('Выберите модели для транскрибации:'),\n",
    "    *model_checkboxes.values()\n",
    "])\n",
    "\n",
    "display(VBox([\n",
    "    Label('Загрузите аудио файл:'),\n",
    "    audio_uploader,\n",
    "    Label('Загрузите эталонный текст (опционально):'),\n",
    "    reference_uploader,\n",
    "    model_selection,\n",
    "    preprocessing_checkbox,\n",
    "    process_btn,\n",
    "    status_label\n",
    "]))\n",
    "\n",
    "def on_process_click(btn):\n",
    "    status_label.value = ''\n",
    "    \n",
    "    try:\n",
    "        if not audio_uploader.value:\n",
    "            raise ValueError(\"Сначала загрузите аудио файл!\")\n",
    "\n",
    "        selected_models = {name: models_mapping[name] for name, cb in model_checkboxes.items() if cb.value}\n",
    "        if not selected_models:\n",
    "            raise ValueError(\"Выберите хотя бы одну модель для транскрибации!\")\n",
    "\n",
    "        audio_file = audio_uploader.value[0]\n",
    "        file_name = audio_file['name']\n",
    "        temp_path = f\"./{file_name}\"\n",
    "        \n",
    "        with open(temp_path, 'wb') as f:\n",
    "            f.write(audio_file['content'])\n",
    "            \n",
    "        if not file_name.lower().endswith('.wav'):\n",
    "            status_label.value = 'Конвертация в WAV...'\n",
    "            file_path = convert_to_wav(temp_path)\n",
    "            os.remove(temp_path)\n",
    "        else:\n",
    "            file_path = temp_path\n",
    "        \n",
    "        if preprocessing_checkbox.value:\n",
    "            status_label.value = 'Предобработка аудио...'\n",
    "            processed_path = apply_audio_preprocessing(file_path)\n",
    "            os.remove(file_path)\n",
    "            file_path = processed_path\n",
    "        \n",
    "        audio_duration = get_audio_duration(file_path)\n",
    "        \n",
    "        status_label.value = 'Диаризация...'\n",
    "        diarization_result = diarize_audio(file_path)\n",
    "        all_results = {'diarization': diarization_result}\n",
    "\n",
    "        reference_text = None\n",
    "        if reference_uploader.value:\n",
    "            ref_file = reference_uploader.value[0]\n",
    "            reference_text = bytes(ref_file['content']).decode('utf-8')\n",
    "\n",
    "        for model_name, transcribe_fn in selected_models.items():\n",
    "            try:\n",
    "                status_label.value = f'Обработка {model_name}...'\n",
    "                clear_memory()\n",
    "                \n",
    "                transcription = transcribe_fn(file_path)\n",
    "                matched_segments = match_speakers(\n",
    "                    transcription['segments'], \n",
    "                    diarization_result['diarization']\n",
    "                )\n",
    "                \n",
    "                metrics = None\n",
    "                if reference_text:\n",
    "                    metrics = calculate_metrics(reference_text, transcription['text'])\n",
    "                \n",
    "                all_results[model_name] = {\n",
    "                    'segments': matched_segments,\n",
    "                    'time': transcription.get('time', 0),\n",
    "                    'metrics': metrics,\n",
    "                    'device': transcription.get('device', 'N/A')\n",
    "                }\n",
    "                \n",
    "                clear_memory()\n",
    "                \n",
    "            except Exception as e:\n",
    "                clear_memory()\n",
    "                all_results[model_name] = {\n",
    "                    'error': f\"{str(e)}\\n{traceback.format_exc()}\",\n",
    "                    'time': 0,\n",
    "                    'device': 'N/A'\n",
    "                }\n",
    "                continue\n",
    "        \n",
    "        status_label.value = 'Генерация отчета...'\n",
    "        output_file = save_combined_results(\n",
    "            file_name,\n",
    "            all_results,\n",
    "            diarization_result['time'],\n",
    "            audio_duration,\n",
    "            preprocessing_checkbox.value\n",
    "        )\n",
    "        \n",
    "        status_label.value = f'Готово! Результат: {output_file}'\n",
    "    \n",
    "    except Exception as e:\n",
    "        status_label.value = f'Критическая ошибка: {str(e)}'\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        if 'file_path' in locals() and os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "        clear_memory()\n",
    "\n",
    "process_btn.on_click(on_process_click)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee082c92",
   "metadata": {},
   "source": [
    "### Уведомление об окончании работы в телеграм (опционально)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625cfb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def send_telegram_message(token, chat_id, text):\n",
    "    url = f\"https://api.telegram.org/bot{token}/sendMessage\"\n",
    "    params = {\n",
    "        \"chat_id\": chat_id,\n",
    "        \"text\": text,\n",
    "        \"parse_mode\": \"HTML\" \n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, data=params)\n",
    "        result = response.json()\n",
    "        if result.get(\"ok\"):\n",
    "            print(\"Сообщение успешно отправлено!\")\n",
    "        else:\n",
    "            print(\"Ошибка отправки:\", result.get(\"description\"))\n",
    "    except Exception as e:\n",
    "        print(\"Критическая ошибка:\", e)\n",
    "\n",
    "TOKEN = #Токен телеграм-бота\n",
    "CHAT_ID = #ID телеграм-чата для отправки\n",
    "MESSAGE_TEXT = \"Транскрибация завершена!!!\"\n",
    "\n",
    "send_telegram_message(TOKEN, CHAT_ID, MESSAGE_TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5369112",
   "metadata": {},
   "source": [
    "## Дополнительные инструменты работы (опционально)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3515d92",
   "metadata": {},
   "source": [
    "### Объединение нескольких аудиофайлов в один"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a3994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import Tk, filedialog, Listbox, Button, Frame, END\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "class AudioMerger:\n",
    "    def __init__(self):\n",
    "        self.root = Tk()\n",
    "        self.root.title(\"Audio Merger\")\n",
    "        self.file_paths = []\n",
    "        \n",
    "        self.listbox = Listbox(self.root, width=80, height=15)\n",
    "        self.frame = Frame(self.root)\n",
    "        \n",
    "        Button(self.frame, text=\"Добавить файлы\", command=self.add_files).pack(side=\"left\")\n",
    "        Button(self.frame, text=\"Вверх\", command=self.move_up).pack(side=\"left\")\n",
    "        Button(self.frame, text=\"Вниз\", command=self.move_down).pack(side=\"left\")\n",
    "        Button(self.frame, text=\"Объединить\", command=self.merge_files).pack(side=\"left\")\n",
    "        \n",
    "        self.frame.pack(pady=10)\n",
    "        self.listbox.pack(pady=5, padx=10)\n",
    "        \n",
    "    def add_files(self):\n",
    "        new_files = filedialog.askopenfilenames(\n",
    "            title=\"Выберите аудиофайлы\",\n",
    "            filetypes=[(\"Audio files\", \"*.wav *.mp3 *.ogg *.flac *.m4a *.mp4\"), (\"All files\", \"*.*\")]\n",
    "        )\n",
    "        for f in new_files:\n",
    "            self.listbox.insert(END, f)\n",
    "    \n",
    "    def move_up(self):\n",
    "        pos = self.listbox.curselection()\n",
    "        if pos and pos[0] > 0:\n",
    "            index = pos[0]\n",
    "            text = self.listbox.get(index)\n",
    "            self.listbox.delete(index)\n",
    "            self.listbox.insert(index-1, text)\n",
    "            self.listbox.select_set(index-1)\n",
    "    \n",
    "    def move_down(self):\n",
    "        pos = self.listbox.curselection()\n",
    "        if pos and pos[0] < self.listbox.size()-1:\n",
    "            index = pos[0]\n",
    "            text = self.listbox.get(index)\n",
    "            self.listbox.delete(index)\n",
    "            self.listbox.insert(index+1, text)\n",
    "            self.listbox.select_set(index+1)\n",
    "    \n",
    "    def merge_files(self):\n",
    "        self.file_paths = list(self.listbox.get(0, END))\n",
    "        if not self.file_paths:\n",
    "            print(\"Нет файлов для объединения!\")\n",
    "            return\n",
    "        \n",
    "        combined = AudioSegment.empty()\n",
    "        \n",
    "        try:\n",
    "            for file_path in self.file_paths:\n",
    "                audio = AudioSegment.from_file(file_path)\n",
    "                combined += audio\n",
    "                print(f\"Обработан: {os.path.basename(file_path)}\")\n",
    "            \n",
    "            output_path = filedialog.asksaveasfilename(\n",
    "                title=\"Сохранить объединенный файл\",\n",
    "                defaultextension=\".m4a\",\n",
    "                filetypes=[\n",
    "                    (\"MPEG-4 Audio (AAC)\", \"*.m4a\"),\n",
    "                    (\"MP4\", \"*.mp4\"),\n",
    "                    (\"MP3\", \"*.mp3\"),\n",
    "                    (\"WAV\", \"*.wav\"),\n",
    "                    (\"Все файлы\", \"*.*\")\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            if output_path:\n",
    "                file_ext = output_path.split('.')[-1].lower()\n",
    "                format_params = {}\n",
    "                \n",
    "                if file_ext in ['mp4', 'm4a']:\n",
    "                    format_params = {\n",
    "                        'format': 'mp4',\n",
    "                        'codec': 'aac',\n",
    "                        'bitrate': '192k'\n",
    "                    }\n",
    "                elif file_ext == 'mp3':\n",
    "                    format_params = {\n",
    "                        'format': 'mp3',\n",
    "                        'bitrate': '192k'\n",
    "                    }\n",
    "                \n",
    "                combined.export(output_path, **format_params)\n",
    "                print(f\"Файл сохранен: {output_path}\")\n",
    "                print(\"Объединение завершено успешно!\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка: {str(e)}\")\n",
    "    \n",
    "    def run(self):\n",
    "        self.root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merger = AudioMerger()\n",
    "    merger.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec0d173",
   "metadata": {},
   "source": [
    "### Рассчет метрик качества транскрибации (без основного пайплайна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ca037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import traceback\n",
    "from ipywidgets import widgets, Button, VBox, HBox, Label\n",
    "from pymystem3 import Mystem\n",
    "from jiwer import wer, cer, mer, wil\n",
    "\n",
    "\n",
    "mystem = Mystem()\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.replace('SPEAKER_00', ' ')\n",
    "    text = text.replace('SPEAKER_01', ' ')\n",
    "    text = text.replace('UNKNOWN', ' ') \n",
    "    text = text.lower().replace('ё', 'е')\n",
    "    text = re.sub(r'[-–]', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    try:\n",
    "        return ''.join(mystem.lemmatize(text)).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка лемматизации: {str(e)}\")\n",
    "        return text\n",
    "\n",
    "\n",
    "def calculate_metrics(reference, hypothesis):\n",
    "    def safe_lemmatize(text):\n",
    "        try:\n",
    "            return lemmatize_text(text) if text else \"\"\n",
    "        except:\n",
    "            return text\n",
    "    \n",
    "    ref_norm = normalize_text(reference or \"\")\n",
    "    hyp_norm = normalize_text(hypothesis or \"\")\n",
    "    ref_lemma = safe_lemmatize(ref_norm)\n",
    "    hyp_lemma = safe_lemmatize(hyp_norm)\n",
    "\n",
    "    def calculate_level_metrics(ref, hyp):\n",
    "        if not ref or not hyp:\n",
    "            return {'WER': 1.0, 'CER': 1.0, 'MER': 1.0, 'WIL': 1.0, 'PIWER': 1.0}\n",
    "        \n",
    "        return {\n",
    "            'WER': wer(ref, hyp),\n",
    "            'CER': cer(ref, hyp),\n",
    "            'MER': mer(ref, hyp),\n",
    "            'WIL': wil(ref, hyp),\n",
    "            'PIWER': 1 - len(set(ref.split()) & set(hyp.split())) / max(1, len(set(ref.split())))\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'Base': calculate_level_metrics(ref_norm, hyp_norm),\n",
    "        'Lemmatized': calculate_level_metrics(ref_lemma, hyp_lemma)\n",
    "    }\n",
    "\n",
    "\n",
    "metric_ref_uploader = widgets.FileUpload(\n",
    "    accept='text/*', \n",
    "    multiple=False, \n",
    "    description='Оригинал'\n",
    ")\n",
    "metric_hyp_uploader = widgets.FileUpload(\n",
    "    accept='text/*', \n",
    "    multiple=False, \n",
    "    description='ML-Транскрипт'\n",
    ")\n",
    "calc_metrics_btn = Button(\n",
    "    description=\"Вычислить метрики\", \n",
    "    button_style='info'\n",
    ")\n",
    "metrics_result_label = Label(value='')\n",
    "\n",
    "\n",
    "def display_metrics(metrics):\n",
    "    result = \"МЕТРИКИ КАЧЕСТВА:\"\n",
    "    result += \"{:<15} {:<10} {:<10} {:<10} {:<10} {:<10}\\n\".format(\n",
    "        \"Type\", \"WER\", \"CER\", \"MER\", \"WIL\", \"PIWER\")\n",
    "    \n",
    "    for level in ['Base', 'Lemmatized']:\n",
    "        result += \"{:<15} {:<10.2%} {:<10.2%} {:<10.2%} {:<10.2%} {:<10.2%}\".format(\n",
    "            level,\n",
    "            metrics[level]['WER'],\n",
    "            metrics[level]['CER'],\n",
    "            metrics[level]['MER'],\n",
    "            metrics[level]['WIL'],\n",
    "            metrics[level]['PIWER']\n",
    "        )\n",
    "    return result\n",
    "\n",
    "\n",
    "def on_calc_metrics_click(btn):\n",
    "    try:\n",
    "        if not metric_ref_uploader.value or not metric_hyp_uploader.value:\n",
    "            raise ValueError(\"Загрузите оба текстовых файла!\")\n",
    "\n",
    "\n",
    "        ref_content = bytes(metric_ref_uploader.value[0]['content']).decode('utf-8')\n",
    "        hyp_content = bytes(metric_hyp_uploader.value[0]['content']).decode('utf-8')\n",
    "\n",
    "        metrics = calculate_metrics(ref_content, hyp_content)\n",
    "        \n",
    "\n",
    "        metrics_result_label.value = display_metrics(metrics)\n",
    "        \n",
    "    except Exception as e:\n",
    "        metrics_result_label.value = f\"Ошибка: {str(e)}\"\n",
    "        traceback.print_exc()\n",
    "\n",
    "calc_metrics_btn.on_click(on_calc_metrics_click)\n",
    "\n",
    "\n",
    "display(VBox([\n",
    "    Label('Сравнение двух текстовых файлов:'),\n",
    "    HBox([metric_ref_uploader, metric_hyp_uploader]),\n",
    "    calc_metrics_btn,\n",
    "    metrics_result_label\n",
    "]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
